---
title: "Making Machine Learning Fast and Interpretable"
subtitle: "Methods and Application to the Prediction of Incident Heart Failure"
author: "Byron C. Jaeger"
date: "PHS Grand Rounds: Sept. 8, 2022"
output:
  xaringan::moon_reader:
    css: [default]
    lib_dir: libs
    nature:
      beforeInit: "macros.js"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
  
---

```{r, include=FALSE, echo=FALSE}

library(tidyverse)
library(lubridate)
library(gt)
library(table.glue)
library(palmerpenguins)
library(ggforce)
library(rpart)
library(party)
library(rpart.plot)
library(parttree)

knitr::opts_chunk$set(echo = FALSE, 
                      message = FALSE,
                      warning = FALSE,
                      dpi = 300,
                      fig.height = 7, 
                      fig.width = 11,
                      fig.align = 'center')

thm  <- theme_bw() + 
  theme(
    text = element_text(size=18, face = 'bold'),
    panel.grid = element_blank()
  )

theme_set(thm)

source("R/bench_pred_viz.R")

```


```{css, echo = FALSE}
.huge { font-size: 200% }
.large { font-size: 130% }
.small { font-size: 70% }
```

# Hello! My name is Byron 

.left-column[

I am an R enthusiast

I like walking my dog and learning (easy) cocktail recipes.

I study <br/> risk prediction using machine learning, generally cardiovascular disease.

]

.right-column[

<img src="img/run_R_kids.png" width="100%" align="right" />

]

---
class: center, middle, inverse

# .huge[Background]

---

## Two types of machine learning

.pull-left[

### Supervised

.large[Labeled data]

.large[Predict an outcome]

.large[Learners]


]

.pull-right[

### Unsupervised

.large[Unlabeled data]

.large[Find structure]

.large[Clusters]

.large[Organize medical records]


]

---

## For today

.pull-left[

### Supervised

.large[Labeled data]

.large[Predict an outcome]

.large[Learners (decision trees & random forests)]

.large[Risk prediction]

]

---
class: center, middle

# Labeled data

---
layout: true
## Labeled data

---

```{r out.width='90%', echo=FALSE, fig.align='center'}
knitr::include_graphics('img/ml-supervised-1.svg')
```

---


```{r out.width='90%', echo=FALSE, fig.align='center'}
knitr::include_graphics('img/ml-supervised-1-1.svg')
```

---

```{r out.width='90%', echo=FALSE, fig.align='center'}
knitr::include_graphics('img/ml-supervised-1-2.svg')
```

---

```{r out.width='90%', echo=FALSE, fig.align='center'}
knitr::include_graphics('img/ml-supervised-1-3.svg')
```

---

```{r out.width='90%', echo=FALSE, fig.align='center'}
knitr::include_graphics('img/ml-supervised-2.svg')
```

---

```{r out.width='90%', echo=FALSE, fig.align='center'}
knitr::include_graphics('img/ml-supervised-3.svg')
```

---

```{r out.width='90%', echo=FALSE, fig.align='center'}
knitr::include_graphics('img/ml-supervised-4.svg')
```

---

```{r out.width='90%', echo=FALSE, fig.align='center'}
knitr::include_graphics('img/ml-supervised-5.svg')
```

---

```{r out.width='90%', echo=FALSE, fig.align='center'}
knitr::include_graphics('img/ml-supervised-8.svg')
```

---
layout: false
class: center, middle

# Predict an outcome

---
layout: true

## Predict an outcome

---

```{r out.width='90%', echo=FALSE, fig.align='center'}
knitr::include_graphics('img/ml-supervised-6.svg')
```

---

```{r out.width='90%', echo=FALSE, fig.align='center'}
knitr::include_graphics('img/ml-supervised-7.svg')
```

---

```{r out.width='90%', echo=FALSE, fig.align='center'}
knitr::include_graphics('img/ml-supervised-7-1.svg')
```

---

```{r out.width='90%', echo=FALSE, fig.align='center'}
knitr::include_graphics('img/ml-supervised-7-2.svg')
```

---
layout: false
class: center, middle, inverse

# .huge[Learners]

--

## .huge[(jargon)]

---

## Learners


.large[A *learner* is like a recipe for a prediction model]

--

- .large[A learner is not the same thing as a prediction model]

- .large[A recipe is not the same thing as the food!]


---
class: center, middle, inverse

# .huge[Risk Prediction]

---

## Risk prediction

.large[__The goal__: predict the probability that a person will experience an event within a specific amount of time.]

.large[__The intent__: direct treatment to those who are most likely to benefit]

.large[__The result (in a perfect world)__:]

- .large[Shared decision-making between clinicians and patients]

- .large[Greater net benefit]

- .large[Lower number needed to treat]

---

## Risk prediction models

.large[__Traditional risk prediction equations__]

- .large[Cox proportional hazards]

- .large[fully parametric survival models] 

.large[__Risk prediction using machine learning__]

- .large[random forests]

- .large[boosted tree ensembles]

- .large[neural networks]


--

<br>
.huge[Machine learning in healthcare?]

---
background-image: url(img/meme_chicken_ml.jpg)
background-size: contain

---
background-image: url(img/segar_et_al.svg)
background-size: contain

---
background-image: url(img/segar_et_al_2.svg)
background-size: contain

---
background-image: url(img/segar_et_al_3.svg)
background-size: contain

---

## My perspective

Dr. Segar compared several machine learning algorithms using a standard and objective procedure to find the algorithm that gave the most accurate predictions.

--

The machine learning algorithm that won this benchmark was the oblique random survival forest (ORSF), a model I developed in 2019 ðŸ˜².

--

Dr. Segar _almost_ didn't use my model!

- It took a very long time to run

- It didn't have a good method to estimate variable importance (we found a workaround for this)

---

## Lessons learned

I learned ORSF was a good model, but 

- nobody likes slow R packages

- interpretation is key

So I decided I needed to make an R package for ORSF that was fast and interpretable.

---
background-image: url(img/aorsf_website.svg)
background-size: contain

---

# This is who I am now 

.left-column[

I am an ~~R~~ `aorsf` enthusiast

I like `aorsf` ~~walking my dog and learning (easy) cocktail recipes.~~

I study `aorsf`<br/> ~~risk prediction using machine learning, generally cardiovascular disease.~~

]

.right-column[

<img src="img/run_R_kids_2.png" width="100%" align="right" />

]


---

## What is a good learner for this?

```{r}

set.seed(329)

n <- 50

x <- runif(n, min = 100, max = 160)

y_true <- scales::rescale(exp(-x/20), to = c(1, 12)) 

y_obs <- pmax(pmin(y_true + rnorm(n, mean=1), 15), 1)

ggdata <- tibble(x = x, y = y_obs) %>% 
 add_row(x = 120, y = 4)

p <- ggplot(ggdata) + 
 aes(x = x, y = y) + 
 labs(y = "Time until next blood pressure measure, months",
      x = "Systolic blood pressure today, mm Hg") + 
 scale_y_continuous(limits = c(-2, 15),
                    breaks = c(0, 2, 4, 6, 8, 10, 12, 14)) + 
 scale_x_continuous(limits = c(100, 160),
                    breaks = c(100, 120, 140, 160))

points <- geom_point(shape = 21, 
                     color = 'black', 
                     fill = 'orange', 
                     size = 4.5)

points_light <- geom_point(shape = 21, 
                           color = 'black', 
                           fill = 'orange', 
                           size = 4.5, 
                           alpha = 0.25)


p + points

m1 <- lm(y_obs ~ x)

x_new <- data.frame(x = seq(100, 160, len = 1000))

yhat_m1 <- m1 %>% 
 predict(newdata = x_new)

m2 <- lm(y_obs ~ poly(x, 3))

yhat_m2 <- m2 %>% 
 predict(newdata = x_new)


```

---

## Learner 1: find the line of best fit


```{r}

p + points + 
 geom_line(data = tibble(y = yhat_m1, x = x_new$x),
           color = 'purple', 
           size = 1.5)

```

---

## Learner 1: find the line of best fit

```{r}

library(magick)
library(ggforce)

raster <- image_read('img/person-standing.png') %>% 
 image_fill('none') %>% 
 as.raster()

circle_data <- tibble(
 x = 120,
 y = predict(m1, newdata = tibble(x=x)),
 label = paste("Our model's prediction\nfor Bill:", 
               round(y, 1), 'months')
)

p + points_light + 
 geom_line(data = tibble(y = yhat_m1, x = x_new$x),
              color = 'purple', 
              size = 1.5) + 
 geom_mark_circle(data = circle_data, 
                  aes(label = label), 
                  fill = 'orange', 
                  label.fontsize = 15,
                  expand = 0.02, 
                  label.fill = 'grey90') + 
 geom_segment(x = 120, 
              y = 2, 
              xend = 120, 
              yend = 6.3) + 
 annotate(geom = 'text',
          size = 6,
          x = 113,
          y = 1.5, 
          label = "Bill's SBP:\n120 mm Hg") +
 annotation_raster(raster, 
                   ymin = -2.2, 
                   ymax = 1.8, 
                   xmin = 115, 
                   xmax = 125)

```

---

## Learner 2: find the curve of best fit

```{r}

p + points + 
 geom_line(data = tibble(y = yhat_m2, x = x_new$x),
           color = 'purple', 
           size = 1.5)

```

---

## Learner 2: find the curve of best fit

```{r}

circle_data <- tibble(
 x = 120,
 y = predict(m2, newdata = tibble(x=x)),
 label = paste("Our model's prediction\nfor Bill:", 
               round(y, 1), 'months')
)

p + points_light + 
 geom_line(data = tibble(y = yhat_m2, x = x_new$x),
              color = 'purple', 
              size = 1.5) + 
 geom_mark_circle(data = circle_data, 
                  aes(label = label), 
                  fill = 'orange', 
                  label.fontsize = 15,
                  expand = 0.02, 
                  label.fill = 'grey90') + 
 geom_segment(x = 120, 
              y = 2, 
              xend = 120, 
              yend = 4.7) + 
 annotate(geom = 'text',
          size = 6,
          x = 113,
          y = 1.5, 
          label = "Bill's SBP:\n120 mm Hg") +
 annotation_raster(raster, 
                   ymin = -2.2, 
                   ymax = 1.8, 
                   xmin = 115, 
                   xmax = 125)
 
 

```

---

## Learner 3: connect the dots

```{r}

p3 <- p + points + 
 geom_line(color = 'purple', size = 1.5)

p3

```

---

## Learner 3: connect the dots

```{r}

circle_data$y <- 4

circle_data$label <- 
 "Our model's prediction\nfor Bill: 4 months"

p + points_light +
 geom_line(color = 'purple', 
           size = 1.5) + 
 geom_mark_circle(data = circle_data, 
                  aes(label = label), 
                  fill = 'orange', 
                  label.fontsize = 15,
                  expand = 0.02, 
                  label.fill = 'grey90') + 
 geom_mark_circle(data = circle_data, 
                  aes(label = label), 
                  fill = 'orange', 
                  label.fontsize = 15,
                  expand = 0.02, 
                  label.fill = 'grey90') + 
 
 geom_segment(x = 120, 
              y = 2, 
              xend = 120, 
              yend = 3.1) + 
 annotate(geom = 'text',
          size = 6,
          x = 113,
          y = 1.5, 
          label = "Bill's SBP:\n120 mm Hg") +
 annotation_raster(raster, 
                   ymin = -2.2, 
                   ymax = 1.8, 
                   xmin = 115, 
                   xmax = 125)

```


---

<iframe src="https://app.sli.do/event/skz3VjibsGSTe3YyF7D68v" height="90%" width="100%" frameBorder="0" style="min-height: 560px;" title="Which model?"></iframe>

---

```{r, out.width='80%'}
knitr::include_graphics('img/orsf-grandrounds-q1.png')
```


---
class: center, middle

# Comparing learners

---

```{r out.width='100%', echo=FALSE, fig.align='center'}
knitr::include_graphics('img/ml_abstraction.svg')
```

.footnote[Basic steps of ML. Source: [mlr3book](https://mlr3book.mlr-org.com/basics.html)]

---
class: inverse, center, middle

# Decision Trees and Random Forests

---
background-image: url(img/penguins.png)
background-size: 45%
background-position: 85% 72.5%

## Decision trees

- Frequently used in supervised learning.

- Partitions the space of predictor variables.

- Can be used for classification, regression, and survival analysis. 

.pull-left[
We'll demonstrate the mechanics of decision trees by developing a prediction rule to classify penguin<sup>1</sup> species (chinstrap, gentoo, or adelie) based on bill and flipper length.
]

.footnote[
<sup>1</sup>Data were collected and made available by [Dr. Kristen Gorman](https://www.uaf.edu/cfos/people/faculty/detail/kristen-gorman.php) and the [Palmer Station](https://pal.lternet.edu/), a member of the [Long Term Ecological Research Network](https://lternet.edu/).
]

---

Dimensions for Adelie, Chinstrap and Gentoo Penguins at Palmer Station

```{r fig.align='center', fig.height=5, fig.width=7}

library(palmerpenguins)

ggplot(data = drop_na(penguins)) +
  aes(x = flipper_length_mm, y = bill_length_mm, label = species) +
  geom_point(aes(color = species, shape = species),
             size = 3,
             alpha = 0.8) +
  geom_mark_ellipse(aes(color = species, fill = species), 
                    alpha = 0.075) +
  theme_minimal() +
  scale_color_manual(values = c("darkorange","purple","cyan4")) +
  scale_fill_manual(values = c("darkorange","purple","cyan4")) +
  labs(x = "\nFlipper length, mm",
       y = "Bill length, mm\n",
       color = "Penguin species",
       shape = "Penguin species") +
  coord_cartesian(ylim = c(30, 70),
                  xlim = c(170, 235)) +
  theme(panel.grid = element_blank(),
        legend.position = '')

```

---

Partition all the penguins into flipper length < 207 or â‰¥ 207 mm

```{r fig.align='center', fig.height=5, fig.width=7}

mdl_tree <- rpart(formula = species ~ flipper_length_mm + bill_length_mm,
                  data = penguins, 
                  control = rpart.control(maxdepth = 1))

ggplot(data = penguins) +
  aes(x = flipper_length_mm, y = bill_length_mm, label = species) +
  geom_point(aes(color = species, shape = species),
             size = 3,
             alpha = 0.8) +
  geom_parttree(data = mdl_tree, aes(fill=species), alpha = 0.1) +
  theme_minimal() +
  scale_color_manual(values = c("darkorange","purple","cyan4")) +
  scale_fill_manual(values = c("darkorange","cyan4")) +
  labs(x = "\nFlipper length, mm",
       y = "Bill length, mm\n",
       color = "Penguin species",
       shape = "Penguin species") +
  coord_cartesian(ylim = c(30, 70),
                  xlim = c(170, 235)) +
  theme(panel.grid = element_blank(),
        legend.position = '')
```

---

Partition penguins on the left side into into bill length < 43 or â‰¥ 43 mm

```{r fig.align='center', fig.height=5, fig.width=7}

mdl_tree <- rpart(formula = species ~ flipper_length_mm + bill_length_mm,
                  data = penguins, 
                  control = rpart.control(maxdepth = 2))

penguin_fig_2parts <- ggplot(data = penguins) +
  aes(x = flipper_length_mm, y = bill_length_mm, label = species) +
  geom_point(aes(color = species, shape = species),
             size = 3,
             alpha = 0.8) +
  geom_parttree(data = mdl_tree, aes(fill=species), alpha = 0.1) +
  theme_minimal() +
  scale_color_manual(values = c("darkorange","purple","cyan4")) +
  scale_fill_manual(values = c("darkorange","purple","cyan4")) +
  labs(x = "\nFlipper length, mm",
       y = "Bill length, mm\n",
       color = "Penguin species",
       shape = "Penguin species") +
  coord_cartesian(ylim = c(30, 70),
                  xlim = c(170, 235)) +
  theme(panel.grid = element_blank(),
        legend.position = '')

penguin_fig_2parts

```

---

The same partitions, visualized as a binary tree.

```{r fig.align='center', out.width='100%'}

knitr::include_graphics('img/rpart_plot_classif.png')

# png(res = 300,
#     width = 6,
#     height = 3.75,
#     units = 'in',
#     filename = 'rpart_plot_classif.png')
# 
# rpart.plot::rpart.plot(mdl_tree,
#                        box.palette = list("darkorange","purple","cyan4"))
# 
# dev.off()

```

Node text, top to bottom: predicted class type; predicted class probability; % of data in node;

---

## Random Forest, i.e., lots of trees

Random forests are large sets of de-correlated decision trees 

- random-ness helps de-correlate the trees

--

The forest's prediction is the average prediction taken over all of the trees, i.e., 

$$
\frac{\text{p}_1 + \cdots + \text{p}_n}{n}
$$
where $p_i$ is the prediction from tree $i$

**Variable importance**: Randomly permute a predictor and re-calculate the forest's prediction error. Noising up an important predictor will make the forest's prediction error go up a lot more than an unimportant predictor.

---
class: inverse, center, middle

# Oblique random survival forest (ORSF)

--

## (Not as complicated as it sounds)

---

A classification decision tree's predictions are the average of the observations in each leaf

```{r fig.align='center', out.width='100%'}

knitr::include_graphics('img/rpart_plot_classif.png')

```

---

With survival trees, the 'average of the observations' is a survival curve.

```{r fig.align='center', out.width='100%'}

knitr::include_graphics('img/rpart_plot_surv.png')

# penguins_sim <- penguins %>%
#  drop_na() %>%
#   mutate(
#     time_mean = if_else(
#       flipper_length_mm < 207,
#       true = 800,
#       false = 200
#     ),
#     time_mean = if_else(
#      bill_length_mm < 43 & flipper_length_mm < 207,
#      true = 500,
#      false = time_mean
#     ),
#     time = rnorm(n = n(), mean = time_mean, sd = 150),
#     time = pmax(time, 50),
#     status = rbinom(n = n(), size = 1, prob = 0.75)
#   )
# 
# library(party)
# 
# png(res = 300,
#     width = 6,
#     height = 4,
#     units = 'in',
#     filename = 'img/rpart_plot_surv.png')
# 
# mdl_ctree <-
#   ctree(formula = Surv(time, status) ~ bill_length_mm +
#          flipper_length_mm,
#         data = penguins_sim,
#         controls = ctree_control(maxdepth = 2))
# 
# plot(
#   mdl_ctree,
#   inner_panel = node_inner(mdl_ctree, pval = FALSE)
# )
# 
# dev.off()

```

.footnote[the survival outcomes are simulated]

---

oblique trees use linear combinations of variables to partition data.

- axis based split: $x_1 < 10$

- oblique split: $0.25 \cdot x_1 + 1.5 \cdot x_2 < 10$

```{r out.width='100%'}
knitr::include_graphics("img/axis_versus_oblique.png")
```

Both trees partition the predictor space defined by variables $x_1$ and $x_2$, but the oblique splits do a better job of separating the two classes.

---

## How I made ORSF faster

```{r out.width='65%'}
knitr::include_graphics('img/orsf_algo_table.svg')
```

---

## How I made ORSF faster

```{r out.width='65%'}
knitr::include_graphics('img/orsf_algo_table_2.svg')
```

---
background-image: url(img/aorsf-arxiv-paper.png)
background-size: 85%

## [aorsf paper](https://arxiv.org/abs/2208.01129)

---

## Negation importance

Permuting the values of predictors to measure variable importance

- works great when a single variable is used to partition data

- sometimes makes no difference for oblique partitions because

    + multiple variables are being used
    
    + the given variable may have a large or small coefficient

--

So instead of permuting the variable's values, why don't we just multiply all of its coefficients by -1?

- puts more emphasis on variables with large coefficients in linear combinations

- inherits all of the nice things about permutation importance.

- works for __any oblique random forest__, not just the survival kind!

---

class: center, middle

# .huge[Better, Faster, more\nInterpretable]

---

## Learners for censored outcomes

```{r,echo=FALSE, cache=TRUE, warning=FALSE}

gg_data <- tribble(
 ~ package,       ~ learners,
 'xgboost',         'boosted trees',
 'ranger',          'random forest',
 'glmnet',          'penalized regression',
 'party',           'conditional inference forest',
 'randomForestSRC', 'random forest',
 'survivalmodels',  'neural networks',
 'obliqueRSF',      'oblique random survival forest'
) %>%
 mutate(
  downloads = map(
   package,
   .f = cranlogs::cran_downloads,
   from = today() - years(3),
   to = today()
  )
 )

gg_plots <- gg_data %>%
 mutate(downloads = map(downloads, select, -package)) %>%
 unnest(downloads) %>%
 filter(count > 0) %>%
 group_by(package) %>%
 mutate(cm = data.table::frollmean(count, n = 100)) %>%
 nest() %>%
 mutate(
  plot = map(
   .x = data,
   .f =  ~ ggplot(drop_na(.x)) +
   aes(x = date, y = cm) +
   geom_point(alpha = 1/2) +
   geom_smooth(method = 'lm',
               formula = y ~ poly(x, 3),
               se = FALSE,
               color = 'red') +
   scale_y_log10() +
   theme_void()
  )
 ) %>%
 select(-data)

gt_data <- gg_data %>%
 mutate(
  downloads = map_dbl(downloads, ~sum(.x$count)),
  fig = NA
 ) %>%
 left_join(gg_plots, by = 'package') %>%
 arrange(desc(downloads)) %>%
 mutate(
  downloads = table.glue::table_value(as.integer(downloads))
 )

gt_data %>%
 select(-plot) %>%
 gt(rowname_col = 'package') %>%
 text_transform(
  locations = cells_body(columns = fig),
  fn = function(x) {
   map(
    .x = gt_data$plot,
    .f = ggplot_image,
    height = px(40),
    aspect_ratio = 3
   )
  }
 ) %>%
 tab_stubhead(label = 'R Package') %>%
 cols_label(fig = 'Trends over time',
            learners = 'Learner',
            downloads = 'Total downloads') %>%
 cols_align('left', columns = c('learners', 'downloads')) %>%
 tab_options(table.width = pct(100)) %>%
 tab_source_note(
  paste("Data span from", today() - years(2),
        "to", today())
 )

```

```{r}

# prep the bm results ----

bm_pred_clean <- read_rds('data/bm_pred_clean.rds')

model_key <- read_rds("data/model_key.rds") %>% 
 mutate(
  label                    = recode(
   label,
   'obliqueRSF-net' = 'obliqueRSF',
   'aorsf-fast' = 'aorsf',
   'xgboost-cox' = 'xgboost',
   'glmnet-cox' = 'glmnet',
   'rsf-standard' = 'randomForestSRC',
   'ranger-extratrees' = 'ranger',
   'cif-standard' = 'party',
   'nn-cox' = 'neural network' 
  )
 )

data_key <- read_rds("data/data_key.rds")

fig_subsets <- list(
 slide_one = c('obliqueRSF', 
               'randomForestSRC',
               'ranger', 
               'party'),
 slide_two = c('obliqueRSF', 
               'aorsf', 
               'randomForestSRC',
               'ranger', 
               'party')
)

fig_chicago <- bm_pred_clean %>% 
 bench_pred_viz_chicago(data_key, 
                        model_key, 
                        model_subset = c("aorsf_fast", "ranger"))

fig_bm <- bm_pred_clean %>% 
 bench_pred_visualize(data_key, model_key)

```

---

## A benchmark

- Compare the computational efficiency and prediction accuracy of `aorsf` to the leading R packages

    + 21 unique datasets containing 35 unique risk prediction tasks

- Compare negation VI to the leading methods for interpreting random forests (and other types of algorithms)

    + simulation study

---

## Zooming in

- One task: predicting mortality risk for patients with primary biliary cholangitis.

- Two learners: `aorsf` and `ranger`

- Performance measure: __Index of prediction accuracy__ (think $R^2$ for risk prediction)

---

## Zooming in

```{r}


data_gg <- bm_pred_clean %>% 
 filter(model %in% c('aorsf_fast', 'ranger'),
        data == 'pbc_orsf') %>% 
 mutate(model = recode(model, !!!deframe(model_key))) 

medians <- data_gg %>% 
 group_by(model) %>% 
 summarize(ibs_scaled = median(ibs_scaled))
 
ggplot(data_gg) + 
 aes(x = model, y = ibs_scaled, fill = model) + 
 geom_jitter(size = 2, shape = 21, color = 'grey') + 
 geom_boxplot(alpha = 0.25) +
 scale_fill_manual(values = c('purple', 'orange')) +
 theme(legend.position = '') + 
 labs(x = '', y = 'Index of prediction accuracy') + 
 scale_y_continuous(breaks = medians$ibs_scaled,
                    labels = round(medians$ibs_scaled*100))
 

```



---

```{r fig.height=9}

fig_chicago$small

```

---

```{r fig.height=9}

fig_chicago$big

```

---

```{r fig.height=9}

fig_bm$fig

```

---
class: center, middle

# .huge[BONUS ROUND]

---

## 2017 ACC/AHA High ASCVD risk

__Definition__: clinical CVD or __predicted 10-year ASCVD risk â‰¥ 10%__

As a matter of practical convenience, adults with stage 1 hypertension and 

- diabetes mellitus, 

- CKD (chronic kidney disease), or 

- â‰¥ 65 years of age 

are considered to have high ASCVD risk by the guideline. 

This recommendation was based on the assumption that most adults with diabetes, CKD, or â‰¥ 65 years of age are likely to have a 10-year predicted ASCVD risk â‰¥ 10% and data from surveys suggesting most clinicians, including cardiologists, do not assess ASCVD risk using recommended risk prediction tools.

.footnote[
<sup>1</sup>See Sections 9.3, 9.6, and 10.3 of  [the guideline](https://www.ahajournals.org/doi/pdf/10.1161/HYP.0000000000000065)
]

---

## Pooled cohort risk equations

Predict 10-year risk for a first atherosclerotic cardiovascular disease (ASCVD) event.

The American College of Cardiology and American Heart Association (ACC/AHA) recommend using these equations to guide clinical decisions related to the management and treatment of stage 1 hypertension.

--

- Nonpharmalogical therapy for all adults with stage 1 hypertension

- For adults with stage 1 hypertension **and high CVD risk**, nonpharmalogical therapy **and** initiation or intensification of antihypertensive medication

- High CVD risk = clinical CVD or predicted 10-year ASCVD risk â‰¥ 10%

--

Does this recommendation help direct antihypertensive medication to people who are most likely to benefit?

---
background-image: url(img/colantonio_2018.jpg)
background-size: 85%

---
